## 概述

我们将训练分为两个不同的阶段：首先，我们训练一个自编码器，它提供了一个低维的表示空间，它与数据空间在感知上是等价的。重要的是，与以前的工作不同，我们不需要依赖过度的空间压缩，因为我们在学习的潜在空间中训练DM，它在空间维度方面表现出更好的缩放特性。降低的复杂度也为单通道的潜在空间提供了高效的图像生成。我们将生成的模型类称为潜在扩散模型(LDM)。

这种方法的一个显著优点是，我们只需要训练一次通用的自动编码阶段，因此可以重用它进行多次DM训练或探索可能完全不同的任务。这使得针对各种图像到图像和文本到图像任务的大量扩散模型的高效探索成为可能。对于后者，我们设计了一种架构，将转换器连接到DM的UNet主干网，并支持任意类型的基于令牌的调节机制

1. 与纯粹的基于transformer的方法相比，我们的方法对高维数据具有更好的伸缩性，因此可以(a)在压缩级别上工作，提供比以前的工作和(b)可以高效的应用于百万像素图像的高分辨率合成
2. 我们在多个任务(无条件图像合成、修复、随机超分辨率)和数据集上实现了具有竞争力的性能，同时显著降低了计算成本。与基于像素的扩散方法相比，我们也显著降低了推断成本
3. 我们表明，与之前的工作同时学习编解码器架构和基于分数的先验不同，我们的方法不需要对重建和生成能力进行微妙的加权。这确保了非常忠实的重建，并且需要很少的潜在空间的正则化
4. 我们发现，对于超分辨率、修复和语义合成等稠密条件任务，我们的模型可以以卷积的方式应用，并渲染出大的、一致性的图像(1024x1024 px)
5. 此外，我们设计了基于交叉注意力的通用条件化机制，实现了多模态训练。我们使用它来训练类条件、文本到图像和布局到图像模型。

## 本文方法

### 感知图像压缩

1. 通过离开高维图像空间，我们获得了计算效率更高的DM，因为采样是在低维空间上进行的。
2. 我们利用从其UNet架构中继承的DM的归纳偏差，这使得它们对具有空间结构的数据特别有效，从而缓解了以前方法所要求的激进的、质量降低的压缩级别的需要。
3. 最后，我们得到了通用的压缩模型，其隐空间可用于训练多个生成模型，也可用于其他下游应用，如单图像CLIP引导的合成

我们的感知压缩模型基于之前的工作，由一个感知损失和一个基于块的对抗目标组合训练的自编码器组成。这确保了重建仅限于图像流形，增强了局部真实感，避免了仅依靠像元间隔损失(如L2或L1目标)引入的模糊。

给定图片$x \in \mathbb{R}^{H \times W \times 3}$，编码器 $\mathcal{E}$ 将$x$编码为隐变量$z = \mathcal{E} (x)$，解码器$\mathcal{D}$从隐层重建图像，令$\widetilde{x} = \mathcal{D} ( z ) = \mathcal{D} ( \mathcal{E} ( x ) )$，其中$z\in \mathbb{R}^{h \times w \times c}$。重要的是，编码器通过一个因子$f = H / h = W / w$对图像进行下采样，我们研究了不同的降采样因子$f = 2^m，m \in \mathbb{N}$。

为了避免任意的高方差隐空间，我们用两种不同的正则化方法进行实验。第一种变体KL-reg .对学习到的隐层施加一个轻微的接近标准正态的KL惩罚，类似于VAE，而VQ-reg .在解码器中使用一个矢量量化层。这个模型可以解释为一个VQGAN，但是量化层被解码器包含。由于我们后续的DM是针对我们学习到的潜在空间$z = \mathcal{E}$的二维结构进行设计的，因此我们可以使用相对温和的压缩率并实现非常好的重建。因此，我们的压缩模型更好地保留了图像的细节。

### LDM
扩散模型是一种概率模型，旨在通过逐步去噪正态分布变量来学习数据分布$p ( x )$，对应于学习长度为$T$的固定马尔科夫链的逆过程。对于图像合成，最成功的模型依赖于$p ( x )$上的变分下界的一个重加权变量，它反映了去噪得分匹配。这些模型可以被解释为去噪自动编码器$\epsilon_\theta( x_t , t)$的等权重序列。它们被训练来预测其输入$x_t$的去噪变体，其中$x_t$是输入$x$的噪声版本。相应的目标可以简化为:

$L^{simple}_{t-1}=\mathbb{E}_{x_0,\epsilon \sim \mathcal{N}(0,I)}[\lVert \epsilon - \epsilon_\theta (x_t,t)\rVert^2_2]$


潜在表示的生成式建模通过我们训练的由$\mathcal{E}$和$\mathcal{D}$组成的感知压缩模型，我们现在可以获得一个高效的、低维的潜在空间，其中高频的、不可感知的细节被抽象掉。与高维像素空间相比，该空间更适合基于似然的生成模型，因为它们现在可以专注于数据的重要语义位，并且在更低维度、计算效率更高的空间中训练。

我们通过交叉注意力机制增强DMs的底层UNet主干，使其成为更灵活的条件图像生成器，这对于学习各种输入模态的注意力模型是有效的。

为了预处理来自不同模态(如语言提示语)的y，我们引入了一个特定于领域的编码器$\tau_\theta$，将$y$投影到中间表示$\tau_\theta( y )\in \mathbb{R}^{M \times d_\tau}$，然后通过一个交叉注意力层实现，将其映射到UNet的中间层

$$Attention( Q , K , V) = softmax (\frac{QK^T}{\sqrt{d}} ) \cdot V$$

$$Q = W^{(i)}_Q \cdot \phi_i(z_t),K = W^{(i)}_K \cdot \tau_\theta( y ),V = W^{(i)}_V \cdot \tau_\theta( y )$$

其中，$\phi_i(z_t) \in R^{N × d_\epsilon^i}$表示UNet实现$\epsilon_\theta$的(扁平化)中间表示，$W^{(i)}_Q,W^{(i)}_K,W^{(i)}_V$是可学习的投影矩阵。更新LDM优化目标如下：

$$L_{LDM}:=
\mathbb{E}_{\mathcal{E}(x),\epsilon \sim \mathcal{N}(0,I),t}
[\lVert \epsilon -
\epsilon_\theta (z_t,t,\tau_\theta(y))\rVert^2_2]$$

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202408161326360.png)

