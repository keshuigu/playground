> [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
>
> [Attention Is All You Need (Transformer) 论文精读](https://zhuanlan.zhihu.com/p/569527564)

## RNN

在深度学习时代早期，人们使用 RNN（循环神经网络）来处理机器翻译任务。一段输入先是会被预处理成一个 token 序列。RNN 会对每个 token 逐个做计算，并维护一个表示整段文字整体信息的状态。根据当前时刻的状态，RNN 可以输出当前时刻的一个 token。具体来说，在第 $t$ 轮计算中，输入是上一轮的状态 $a^{<t-1>}$ 以及这一轮的输入 token $x^{<t>}$ ，输出这一轮的状态 $a^{<t>}$ 以及这一轮的输出 token $y^{<t>}$ 。

然而，大多数情况下，机器翻译的输出和输入都不是等长的。因此，人们使用了一种新的架构。前半部分的 RNN 只有输入，后半部分的 RNN 只有输出（上一轮的输出会当作下一轮的输入以补充信息）。两个部分通过一个状态
来传递信息。把该状态看成输入信息的一种编码的话，前半部分可以叫做编码器，后半部分可以叫做解码器。这种架构因而被称为编码器-解码器架构

![encoder-decoder](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202403271005912.png)

这种架构存在不足：编码器和解码器之间只通过一个隐状态来传递信息。在处理较长的文章时，这种架构的表现不够理想。为此，有人提出了基于注意力的架构。这种架构依然使用了编码器和解码器，只不过解码器的输入是编码器的状态的加权和，而不再是一个简单的中间状态。每一个输出对每一个输入的权重叫做注意力，注意力的大小取决于输出和输入的相关关系。这种架构优化了编码器和解码器之间的信息交流方式，在处理长文章时更加有效

尽管注意力模型的表现已经足够优秀，但所有基于 RNN 的模型都面临着同样一个问题：RNN 本轮的输入状态取决于上一轮的输出状态，这使 RNN 的计算必须串行执行。因此，RNN 的训练通常比较缓慢

## 注意力机制

放缩点乘注意力（Scaled Dot-Product Attention）：

$$ Attention(Q,K,V)= softmax(\frac{QK^T}{\sqrt{d_k}})V$$

其中， $K$ 是 key 向量的数组，对应需要查询的键； $V$ 是 value 向量的数组，也就是键对应的值； $Q$ 是多次查询的集合； $d_k$ 是 query 和 key 向量的长度，用于放缩 $QK^T$ ，防止值过大。

### 自注意力

自注意力模块的目的是为每一个输入 token 生成一个向量表示，该表示不仅能反映 token 本身的性质，还能反映 token 在句子里特有的性质。

自注意力模块的输入是 3 个矩阵 $Q,K,V$ 。准确来说，这些矩阵是向量的数组，也就是每一个 token 的 query, key, value 向量构成的数组。自注意力模块会为每一个 token 输出一个向量表示 $A$ 。$A^{<t>}$ 是 $t$ 第个 token 在这句话里的向量表示。

### 多头注意力

在自注意力中，每一个单词的 query, key, value 应该只和该单词本身有关。因此，这三个向量都应该由单词的词嵌入得到。另外，每个单词的 query, key, value 不应该是人工指定的，而应该是可学习的。因此，我们可以用可学习的参数来描述从词嵌入到 query, key, value 的变换过程。综上，自注意力的输入 $Q,K,V$ 应该用下面这个公式计算：

$$ \begin{cases} Q=EW^Q \\ K=EW^K \\ V=EW^K \end{cases} $$

其中， $E$ 是词嵌入矩阵，也就是每个单词的词嵌入的数组。 $W^Q,W^K,W^K$ 是可学习的参数矩阵。

就像卷积层能够用多个卷积核生成多个通道的特征一样，我们也用多组 $W^Q,W^K,W^K$ 生成多组自注意力结果。这样，每个单词的自注意力表示会更丰富一点。这种机制就叫做多头注意力。把多头注意力用在自注意力上的公式为：

$$ head_i = Attention(EW_i^Q,EW_i^K,EW_i^V) \\ MultiHeadSelfAttention(E) = Concat(head_i,\dots,head_h)W^O$$

## Transformer 架构

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202403271039292.png)

### 残差连接

Transformer 使用了和 ResNet 类似的残差连接，即设模块本身的映射为 $F(x)$ ，则模块输出为 $Normalization(F(x)+x)$ 。和 ResNet 不同，Transformer 使用的归一化方法是 LayerNorm。

### 前馈网络

架构图中的前馈网络（Feed Forward）其实就是一个全连接网络。具体来说，这个子网络由两个线性层组成，中间用 ReLU 作为激活函数。

$$ FFN(x) = max(0,xW_1+b_1)W_2+b_2 $$

### 整体架构与掩码多头注意力

对于输入序列 $(x_1,\dots,x_s)$ , 它会被编码器编码成中间表示 $z=(z_1,\dots,z_s)$ , 给定 $z$ 的前提下，解码器输入 $(y_1,\dots,y_t)$ , 输出 $(y_2,\dots,y_t+1)$ 的预测值。

具体来说，输入序列 $x$ 会经过 $N=6$ 个结构相同的层。每层由多个子层组成。第一个子层是多头注意力层，准确来说，是多头自注意力。这一层可以为每一个输入单词提取出更有意义的表示。之后数据会经过前馈网络子层。最终，输出编码结果 $z$ 。

得到了 $z$ 后，要用解码器输出结果了。解码器的输入是当前已经生成的序列 $(y_1,\dots,y_t)$ ，该序列会经过一个掩码多头自注意力子层。它的作用和编码器中的一样，用于提取出更有意义的表示。

> 在输出第 $t+1$ 个单词时，模型不应该提前知道 $t$ 时刻之后的信息。因此，应该只保留 $t$ 时刻之前的信息，遮住后面的输入。这可以通过添加掩码实现。

接下来，数据还会经过一个多头注意力层。这个层比较特别，它的 K，V 来自 $z$ ，Q 来自上一层的输出。经过第二个多头注意力层后，和编码器一样，数据会经过一个前馈网络。最终，网络并行输出各个时刻的下一个单词。

### 嵌入层

和其他大多数序列转换任务一样，Transformer 主干结构的输入输出都是词嵌入序列。词嵌入，其实就是一个把 one-hot 向量转换成有意义的向量的转换矩阵。

在 Transformer 中，解码器的嵌入层和输出线性层是共享权重的：输出线性层表示的线性变换是嵌入层的逆变换，其目的是把网络输出的嵌入再转换回 one-hot 向量。如果某任务的输入和输出是同一种语言，那么编码器的嵌入层和解码器的嵌入层也可以共享权重。

### 位置编码

嵌入层的输出是一个向量数组，即词嵌入向量的序列。设数组的位置 $pos$ ，向量的某一维叫 $i$ 。我们为每一个向量里的每一个数添加一个实数编码，这种编码方式要满足以下性质：

1. 对于同一个 $pos$ 的不同的 $i$ ，即对于一个词嵌入向量的不同元素，它们的编码各不相同。
2. 对于向量的同一个维度处，不同 $pos$ 的编码不同。且 $pos$ 间要满足相对关系，即 $f(pos+1)-f(pos) = f(pos) - f(pos-1)$

$$ PE(pos,2i) = \sin(pos/10000^{2i/d_{model}}) \\ PE(pos,2i+1) = \cos(pos/10000^{2i/d_{model}})$$

满足：同pos不同i三角函数值不重复， $f(pos+k)$ 是 $f(pos)$ 的线性函数