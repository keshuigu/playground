# 最优化作业3
## T1
通过迭代与逼近，可以逐步提⾼我们对问题的理解，并逐步接近最优解。具体来说，最优化⽅法通常会
涉及到⼀个⽬标函数，这个函数描述了我们希望优化的量，⽐如说成本、效率等等。⽽为了优化这个⽬
标函数，需要选择⼀些参数或变量，并且不断调整它们，以期望得到更好的结果。在这个过程中，我们
通常会采⽤⼀些迭代的⽅法，例如梯度下降法。梯度下降法是⼀种基于负梯度⽅向更新参数的算法，每
次迭代都会使得⽬标函数值下降⼀定程度。通过多次迭代，我们可以逐步接近最优解。⽽在迭代的过程
中，我们并不知道什么时候能够达到最优解，因此需要采⽤逼近思想。逼近思想就是不断尝试新的参数
或变量值，并评估它们对⽬标函数的影响，直到找到更接近最优解的值为⽌。总的来说，迭代和逼近思
想是最优化⽅法中不可或缺的两个概念，通过不断地调整参数或变量，并不断尝试新的值，我们可以逐
步接近最优解

## T2

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271507076.png)

## T3

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271508365.png)


## T4

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271513026.png)

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271514374.png)

## T5


![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271527774.png)

## T6

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271527195.png)

在最优化中，对偶问题的作用主要体现在以下几个方面：对偶问题提供了原问题的一个下界。对于一个最小化问题，其对偶问题的解总是小于或等于原问题的解，这为我们提供了一个估计或者一个下界。对偶问题可以简化问题的求解。有些时候，原问题可能很难直接求解，但是其对偶问题可能更容易求解。对偶问题可以帮助我们理解原问题的性质。通过研究对偶问题，我们可以得到原问题的一些重要性质，比如可行性、最优性等。

弱对偶定理指出，对于任何原问题的可行解和对偶问题的可行解，原问题的目标函数值总是大于或等于对偶问题的目标函数值。这意味着对偶问题的最优解总是原问题最优解的一个下界。
强对偶定理则进一步指出，在某些条件下（比如Slater条件），原问题的最优解和对偶问题的最优解是相等的。这意味着在这些条件下，我们可以通过求解对偶问题来得到原问题的最优解。

## T7

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271528117.png)

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271535234.png)

## T8

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271534799.png)

## T9

启发式优化算法的基本思想是通过⼀系列迭代搜索过程，在解空间中寻找最优或次优解。这些算法使⽤
启发信息（heuristics）来指导搜索⽅向，从⽽避免在整个解空间内进⾏搜索。启发信息可以是问题领域
的专业知识、经验或者简单的规则。点集匹配问题和拼图重构问题都是组合优化问题，即在⼀个离散的
解空间中搜索最优解。对于这种类型的问题，传统的精确求解⽅法在实际问题中往往⾯临着计算复杂度
过高的挑战。因此，我们可以采⽤启发式算法来解决这些问题。
1. 在点集匹配问题中，我们需要将两组特征点进⾏匹配。⼀种有效的启发式算法是基于局部贪⼼策略的
迭代局部搜索算法。该算法从⼀组初始匹配开始，然后通过不断地调整或交换匹配来改进解。该算法使
⽤了相邻解之间的启发信息，例如误差函数的梯度或者约束条件的变化量，来指导搜索⽅向。由于其算
法复杂度较低，该算法通常可以在多项式时间内完成。
2. 在拼图重构问题中，我们需要将若⼲个拼图碎⽚进⾏组合，以形成完整的图像。⼀种常⽤的启发式算
法是基于遗传算法的优化⽅法。该算法通过模拟⽣物进化过程，采⽤选择、交叉和变异等操作来⽣成新
的解，并使⽤适应度函数来评价解的质量。由于该算法可以同时搜索多个解，因此可以有效地避免陷⼊
局部最优解。然⽽，由于其算法复杂度较⾼，该算法通常需要更⻓的计算时间。
总之，启发式优化算法通过利⽤启发信息来指导搜索⽅向，从⽽提⾼了解决组合优化问题的效率。在实
际问题中，我们可以根据问题的特点选择合适的启发式算法来求解。、


## T10

求解⼤规模问题的分布式优化算法及其基本思想：
1. 并⾏随机梯度下降算法（Parallel Stochastic Gradient Descent，PSGD）：该算法通过将数据划分为
多个部分，每个部分由不同的处理器并⾏处理。它使⽤随机梯度下降的⽅法来更新参数，并使⽤平均梯
度来更新全局参数。该算法适⽤于⼤规模的线性分类或回归问题。
2. ADMM分布式优化算法（Alternating Direction Method of Multipliers Distributed Optimization
Algorithm）：该算法将原始问题转化为⼀个等价的⼦问题序列，并通过交替更新两个变量来求解。每次
迭代需要进⾏局部计算和全局通信。该算法适⽤于带有约束条件的凸优化问题。
3. MapReduce分布式优化算法：该算法将问题划分为多个⼩任务，并将它们分配给不同的处理器来并
⾏处理。每个处理器将结果合并后发送给主处理器进⾏进⼀步的处理。该算法适⽤于那些可以被分解为
多个独⽴的计算任务的问题，例如朴素⻉叶斯分类或K-means聚类。
4. 基于⽜顿法的分布式优化算法：该算法使⽤⽜顿法来求解优化问题，并将计算任务分配给多个处理器
进⾏并⾏计算。使⽤⽜顿法，需要计算海森矩阵和梯度向量。在分布式设置下，每个处理器只负责计算
⼀个⼦集的数据，并将计算结果发送给主处理器进⾏进⼀步的处理。该算法适⽤于⼤规模的⾮线性优化
问题。
总之，分布式优化算法通过将问题划分为多个部分并将它们分配给不同的处理器来并⾏处理，从⽽提⾼
了求解⼤规模问题的效率。这些算法可以根据具体问题的特点进⾏选择和调整。

## T11

罚因⼦⽅法和增⼴拉格朗⽇⽅法都是求解约束优化问题的常⽤算法。罚因⼦⽅法的基本思想是将原始的
带有约束的优化问题转化为⼀个⽆约束的优化问题，通过在⽬标函数中引⼊⼀系列罚项来惩罚不满⾜约
束条件的解，并使⽤优化算法求解。罚因⼦⽅法可以使⽤多种优化算法求解，例如梯度下降、⽜顿法
等。增⼴拉格朗⽇⽅法的基本思想是通过将原始问题的所有约束条件转化为拉格朗⽇乘⼦变量，从⽽得
到⼀个包含拉格朗⽇乘⼦的增⼴拉格朗⽇函数。该函数的极⼩值即为原始问题的最优解。增⼴拉格朗⽇
⽅法可以通过求解增⼴拉格朗⽇函数的偏导数来求解，或者使⽤⾮线性规划求解器等优化算法求解。
ADMM⽅法与增⼴拉格朗⽇⽅法相似，也是通过将原始问题的约束条件转化为拉格朗⽇乘⼦变量。但
是，在ADMM⽅法中，原始问题被转化为⼀个等价的⼦问题序列，每个⼦问题只包含原始问题的部分⽬
标函数及其相关约束条件。每个⼦问题都可以通过局部计算和全局通信的⽅式进⾏求解，从⽽实现并⾏
计算。通过交替更新拉格朗⽇乘⼦和原始变量，最终得到原始问题的最优解。增⼴拉格朗⽇⽅法和
ADMM⽅法的主要区别在于求解过程中是否需要将原始问题转化为⼀个等价的⼦问题序列，并且ADMM
⽅法具有更好的并⾏性能。

## T12

⾮凸问题是指⽬标函数存在多个局部最优解的优化问题。将⾮凸问题转化为凸问题的⽬的是为了能够得
到全局最优解，因为凸问题具有唯⼀的全局最优解。以下是⼏种将⾮凸问题转化为凸问题的⽅法：
1. 对偶问题：通过对原始问题的拉格朗⽇乘⼦进⾏松弛，得到⼀个等价的凸对偶问题。对偶问题可以使
⽤现有的凸优化算法来求解。
2. 分段线性化：将⾮凸⽬标函数分段线性化，得到⼀个包含多个凸⼦问题的优化问题。每个⼦问题都可
以使⽤凸优化算法进⾏求解，并将所有⼦问题的解合并起来形成全局最优解。
3. 修改⽬标函数，使之转化为凸函数
4. 抛弃⼀些约束条件，使新的可⾏域为凸集并且包含原可⾏域