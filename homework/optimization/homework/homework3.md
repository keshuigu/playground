# 最优化作业3
## T1
通过迭代与逼近，可以逐步提高我们对问题的理解，并逐步接近最优解。具体来说，最优化方法通常会
涉及到一个目标函数，这个函数描述了我们希望优化的量，比如说成本、效率等等。而为了优化这个目
标函数，需要选择一些参数或变量，并且不断调整它们，以期望得到更好的结果。在这个过程中，我们
通常会采用一些迭代的方法，例如梯度下降法。梯度下降法是一种基于负梯度方向更新参数的算法，每
次迭代都会使得目标函数值下降一定程度。通过多次迭代，我们可以逐步接近最优解。而在迭代的过程
中，我们并不知道什么时候能够达到最优解，因此需要采用逼近思想。逼近思想就是不断尝试新的参数
或变量值，并评估它们对目标函数的影响，直到找到更接近最优解的值为止。总的来说，迭代和逼近思
想是最优化方法中不可或缺的两个概念，通过不断地调整参数或变量，并不断尝试新的值，我们可以逐
步接近最优解

## T2

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271507076.png)

## T3

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271508365.png)


## T4

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271513026.png)

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271514374.png)

## T5


![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271527774.png)

## T6

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271527195.png)

在最优化中，对偶问题的作用主要体现在以下几个方面：对偶问题提供了原问题的一个下界。对于一个最小化问题，其对偶问题的解总是小于或等于原问题的解，这为我们提供了一个估计或者一个下界。对偶问题可以简化问题的求解。有些时候，原问题可能很难直接求解，但是其对偶问题可能更容易求解。对偶问题可以帮助我们理解原问题的性质。通过研究对偶问题，我们可以得到原问题的一些重要性质，比如可行性、最优性等。

弱对偶定理指出，对于任何原问题的可行解和对偶问题的可行解，原问题的目标函数值总是大于或等于对偶问题的目标函数值。这意味着对偶问题的最优解总是原问题最优解的一个下界。
强对偶定理则进一步指出，在某些条件下（比如Slater条件），原问题的最优解和对偶问题的最优解是相等的。这意味着在这些条件下，我们可以通过求解对偶问题来得到原问题的最优解。

## T7

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271528117.png)

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271535234.png)

## T8

![](https://cdn.jsdelivr.net/gh/keshuigu/images@main/imgs/202404271534799.png)

## T9

启发式优化算法的基本思想是通过一系列迭代搜索过程，在解空间中寻找最优或次优解。这些算法使用
启发信息（heuristics）来指导搜索方向，从而避免在整个解空间内进行搜索。启发信息可以是问题领域
的专业知识、经验或者简单的规则。点集匹配问题和拼图重构问题都是组合优化问题，即在一个离散的
解空间中搜索最优解。对于这种类型的问题，传统的精确求解方法在实际问题中往往⾯临着计算复杂度
过高的挑战。因此，我们可以采用启发式算法来解决这些问题。
1. 在点集匹配问题中，我们需要将两组特征点进行匹配。一种有效的启发式算法是基于局部贪心策略的迭代局部搜索算法。该算法从一组初始匹配开始，然后通过不断地调整或交换匹配来改进解。该算法使用了相邻解之间的启发信息，例如误差函数的梯度或者约束条件的变化量，来指导搜索方向。由于其算法复杂度较低，该算法通常可以在多项式时间内完成。
2. 在拼图重构问题中，我们需要将若干个拼图碎片进行组合，以形成完整的图像。一种常用的启发式算法是基于遗传算法的优化方法。该算法通过模拟生物进化过程，采用选择、交叉和变异等操作来生成新的解，并使用适应度函数来评价解的质量。由于该算法可以同时搜索多个解，因此可以有效地避免陷入局部最优解。然而，由于其算法复杂度较高，该算法通常需要更长的计算时间。总之，启发式优化算法通过利用启发信息来指导搜索方向，从而提高了解决组合优化问题的效率。在实际问题中，我们可以根据问题的特点选择合适的启发式算法来求解。


## T10

求解⼤规模问题的分布式优化算法及其基本思想：
1. 并行随机梯度下降算法（Parallel Stochastic Gradient Descent，PSGD）：该算法通过将数据划分为多个部分，每个部分由不同的处理器并行处理。它使用随机梯度下降的方法来更新参数，并使用平均梯度来更新全局参数。该算法适用于⼤规模的线性分类或回归问题。
2. ADMM分布式优化算法（Alternating Direction Method of Multipliers Distributed Optimization
Algorithm）：该算法将原始问题转化为一个等价的子问题序列，并通过交替更新两个变量来求解。每次迭代需要进行局部计算和全局通信。该算法适用于带有约束条件的凸优化问题。
3. MapReduce分布式优化算法：该算法将问题划分为多个⼩任务，并将它们分配给不同的处理器来并行处理。每个处理器将结果合并后发送给主处理器进行进一步的处理。该算法适用于那些可以被分解为多个独立的计算任务的问题，例如朴素贝叶斯分类或K-means聚类。
4. 基于牛顿法的分布式优化算法：该算法使用牛顿法来求解优化问题，并将计算任务分配给多个处理器进行并行计算。使用牛顿法，需要计算海森矩阵和梯度向量。在分布式设置下，每个处理器只负责计算一个子集的数据，并将计算结果发送给主处理器进行进一步的处理。该算法适用于⼤规模的非线性优化问题。总之，分布式优化算法通过将问题划分为多个部分并将它们分配给不同的处理器来并行处理，从而提高了求解⼤规模问题的效率。这些算法可以根据具体问题的特点进行选择和调整。

## T11

罚因子方法和增广拉格朗日方法都是求解约束优化问题的常用算法。罚因子方法的基本思想是将原始的
带有约束的优化问题转化为一个无约束的优化问题，通过在目标函数中引入一系列罚项来惩罚不满⾜约
束条件的解，并使用优化算法求解。罚因子方法可以使用多种优化算法求解，例如梯度下降、牛顿法
等。增广拉格朗日方法的基本思想是通过将原始问题的所有约束条件转化为拉格朗日乘子变量，从而得
到一个包含拉格朗日乘子的增广拉格朗日函数。该函数的极⼩值即为原始问题的最优解。增广拉格朗日
方法可以通过求解增广拉格朗日函数的偏导数来求解，或者使用非线性规划求解器等优化算法求解。
ADMM方法与增广拉格朗日方法相似，也是通过将原始问题的约束条件转化为拉格朗日乘子变量。但
是，在ADMM方法中，原始问题被转化为一个等价的子问题序列，每个子问题只包含原始问题的部分目
标函数及其相关约束条件。每个子问题都可以通过局部计算和全局通信的方式进行求解，从而实现并行
计算。通过交替更新拉格朗日乘子和原始变量，最终得到原始问题的最优解。增广拉格朗日方法和
ADMM方法的主要区别在于求解过程中是否需要将原始问题转化为一个等价的子问题序列，并且ADMM
方法具有更好的并行性能。

## T12

非凸问题是指目标函数存在多个局部最优解的优化问题。将非凸问题转化为凸问题的目的是为了能够得
到全局最优解，因为凸问题具有唯一的全局最优解。以下是几种将非凸问题转化为凸问题的方法：
1. 对偶问题：通过对原始问题的拉格朗日乘子进行松弛，得到一个等价的凸对偶问题。对偶问题可以使用现有的凸优化算法来求解。
2. 分段线性化：将非凸目标函数分段线性化，得到一个包含多个凸子问题的优化问题。每个子问题都可以使用凸优化算法进行求解，并将所有子问题的解合并起来形成全局最优解。
3. 修改目标函数，使之转化为凸函数
4. 抛弃一些约束条件，使新的可行域为凸集并且包含原可行域